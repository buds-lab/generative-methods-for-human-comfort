{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T12:16:08.431084Z",
     "start_time": "2019-06-27T12:16:08.423218Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pandas, matplotlib, random\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "# Sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T09:41:06.217387Z",
     "start_time": "2019-06-28T09:41:06.197050Z"
    }
   },
   "outputs": [],
   "source": [
    "DEFAULT_K = 10 # default number of folds\n",
    "\n",
    "def holisticsTrainTest(dataframe, list_complete_participants, train_test_split = 0.5, mlp=False, user_split=False):\n",
    "    \"\"\"\n",
    "    Prepare a dataframe and split it into train_test_split. Return both sets.\n",
    "    It's assumed the dataframe does have the participant_no feature\n",
    "    \"\"\"\n",
    "    \n",
    "    df = dataframe.copy()\n",
    "    \n",
    "    if user_split:\n",
    "        random.seed(75)\n",
    "        random.shuffle(list_complete_participants)\n",
    "        random.seed(75)\n",
    "        test_participants = random.sample(set(list_complete_participants), \n",
    "                                          int(round((1 - train_test_split) * len(list_complete_participants))))\n",
    "\n",
    "        print(\"Num participants in test set: \".format(len(test_participants)))\n",
    "\n",
    "        # only pick the train_test_split% of the complete participants for testing\n",
    "        df_test = df[df['Participant_No'].isin(test_participants)]\n",
    "\n",
    "        print(\"Testing on participants:\")\n",
    "        print(df_test['Participant_No'].unique())\n",
    "\n",
    "        # use the rest for training (the negate of above)\n",
    "        df_train = df[~df['Participant_No'].isin(test_participants)]\n",
    "\n",
    "    else:\n",
    "        # shuffle\n",
    "        df = df.sample(frac=1, random_state=100).reset_index(drop=True)\n",
    "\n",
    "        # determine split\n",
    "        idx_split = int(df.shape[0] * train_test_split)\n",
    "\n",
    "        # split the dataframe\n",
    "        df_train = df.iloc[:idx_split, :]\n",
    "        df_test = df.iloc[idx_split:, :]\n",
    "\n",
    "    # removing the participant number since it's a holistic model\n",
    "    del df_test['Participant_No']\n",
    "    del df_train['Participant_No']\n",
    "\n",
    "    # shuffle \n",
    "    df_train = df_train.sample(frac=1, random_state=100).reset_index(drop=True)\n",
    "    df_test = df_test.sample(frac=1, random_state=100).reset_index(drop=True)\n",
    "    \n",
    "    # create binary label versions of the sets\n",
    "    df_train_binary = df_train.copy()\n",
    "    df_test_binary = df_test.copy()\n",
    "    df_train_binary['Discrete Thermal Comfort_TA'] = df_train['Discrete Thermal Comfort_TA'].map(lambda x: 1 if x != 0 else 0)\n",
    "    df_test_binary['Discrete Thermal Comfort_TA'] = df_test['Discrete Thermal Comfort_TA'].map(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "    return df_train, df_test, df_train_binary, df_test_binary\n",
    "\n",
    "def chooseK(train_labels):\n",
    "    \"\"\"\n",
    "    Determine number of folds\n",
    "    \"\"\"\n",
    "    DEFAULT_K = 10\n",
    "    \n",
    "    classCounter = Counter(train_labels)\n",
    "    numLeastCommonClass = min(classCounter.values())\n",
    "    return min(numLeastCommonClass, DEFAULT_K)\n",
    "\n",
    "def getClfMetrics(test_labels, pred_labels):\n",
    "    \"\"\"\n",
    "    Compute different validation metrics for a classification problem.\n",
    "    Metrics:\n",
    "    - micro and macro F1 score\n",
    "    - Confusion Matrix\n",
    "    - Classification Report\n",
    "    \"\"\"\n",
    "    \n",
    "    acc = accuracy_score(test_labels, pred_labels) \n",
    "    print(\"Accuracy (f1 micro) on test set: \", acc)\n",
    "    print(\"F1 micro on test set: \", f1_score(test_labels, pred_labels, average = 'micro'))\n",
    "    print(\"F1 macro on test set: \", f1_score(test_labels, pred_labels, average = 'macro'))\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(confusion_matrix(test_labels, pred_labels)) #, labels = [-2, -1, 0, 1, 2])\n",
    "    print(\"Classification Metrics: \")\n",
    "    print(classification_report(test_labels, pred_labels))\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T09:42:00.019958Z",
     "start_time": "2019-06-28T09:41:59.986227Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculateCost(dataframe, list_complete_participants, temp_high, temp_low, clf=None, occuTherm=False, \n",
    "                  mlp=False, user_split=False, train_test_split = 0.5):\n",
    "    #TODO: could use some cleaning\n",
    "    # resplitting dataset because we need participant id\n",
    "    if user_split:\n",
    "        random.seed(75)\n",
    "        random.shuffle(list_complete_participants)\n",
    "        random.seed(75)\n",
    "        test_participants = random.sample(set(list_complete_participants), \n",
    "                                          int(round((1 - train_test_split) * len(list_complete_participants))))\n",
    "     \n",
    "        # take the same 30% used for data split\n",
    "        df_test = dataframe[dataframe['Participant_No'].isin(test_participants)]\n",
    "    \n",
    "    else:\n",
    "        # shuffle\n",
    "        df = dataframe.sample(frac=1, random_state=100).reset_index(drop=True)\n",
    "\n",
    "        # determine split\n",
    "        idx_split = int(df.shape[0] * train_test_split)\n",
    "\n",
    "        # split the dataframe\n",
    "        df_test = df.iloc[idx_split:, :]\n",
    "\n",
    "    # get list of participants in the test set\n",
    "    participants = list(df_test['Participant_No'].unique())\n",
    "    print(\"Testing on participants:\")\n",
    "    print(df_test['Participant_No'].unique())\n",
    "        \n",
    "    # the temperature range we care about is such that all the baselines predicts comfortable\n",
    "    pred_comfort = 0\n",
    "    \n",
    "    accumulative_rmse = 0 # accumulative RMSE\n",
    "    total_rmse = 0\n",
    "    total_num_responses = 0\n",
    "            \n",
    "    if occuTherm:    \n",
    "        temp_low, temp_high = findTempRangeOccutherm(df_test, clf, mlp)    \n",
    "        print(temp_low)\n",
    "        print(temp_high)\n",
    "    \n",
    "    # for each participant\n",
    "    for p in participants:\n",
    "        curr_participant = df_test[df_test['Participant_No'] == p]\n",
    "        \n",
    "        # RMSE calculation variables\n",
    "        participant_rmse = 0 \n",
    "        participant_num_respones = 0\n",
    "\n",
    "        # if we are evaluating our model, we need to find the comfort temp range for each participant\n",
    "#         if occuTherm:\n",
    "#             curr_p_occutherm = curr_participant.copy()\n",
    "# #             for prediction we don't use these columns\n",
    "#             del curr_p_occutherm['Discrete Thermal Comfort_TA']\n",
    "#             del curr_p_occutherm['Participant_No']\n",
    "\n",
    "#             if mlp:\n",
    "#                 scaler = StandardScaler()\n",
    "#                 curr_p_occutherm_array = scaler.fit_transform(curr_p_occutherm)\n",
    "#                 occutherm_pred = clf.predict(np.array(curr_p_occutherm_array))\n",
    "#                 # remapping y's from one hot encoding from MLP\n",
    "#                 occutherm_pred = np.array([np.argmax(item) for item in occutherm_pred])\n",
    "#                 occutherm_pred = occutherm_pred - 2 # back to original [-2,-1,0,+1,+2]\n",
    "                \n",
    "#             else:\n",
    "#                 # calculate the range of curr_p_occutherm at which occuTherm model predicts 0\n",
    "#                 occutherm_pred = clf.predict(np.array(curr_p_occutherm))\n",
    "            \n",
    "#             occutherm_temps = []\n",
    "#             curr_p_occutherm = curr_p_occutherm.reset_index(drop=True)\n",
    "            \n",
    "#             for index, row in curr_p_occutherm.iterrows():\n",
    "#                 temp = row['Temperature (Fahrenheit)']\n",
    "#                 if occutherm_pred[index] == 0:\n",
    "#                     occutherm_temps.append(temp)\n",
    "\n",
    "#             # if no range for current participant\n",
    "#             if occutherm_temps:\n",
    "#                 temp_low = min(occutherm_temps)  # fahrenheit\n",
    "#                 temp_high = max(occutherm_temps) # fahrenheit\n",
    "#                 print(temp_low)\n",
    "#                 print(temp_high)\n",
    "                     \n",
    "        # iterate through all the participant's responses\n",
    "        for index, row in curr_participant.iterrows():\n",
    "            temp = row['Temperature (Fahrenheit)']\n",
    "            true_comfort = row['Discrete Thermal Comfort_TA']\n",
    "            \n",
    "            # see if temperature falls within range of the baseline\n",
    "            if (temp >= temp_low) and (temp <= temp_high):\n",
    "                # keep track of current participant's cost\n",
    "                participant_rmse = participant_rmse + (pred_comfort - true_comfort) ** 2\n",
    "#                 print((pred_comfort - true_comfort)**2)\n",
    "                participant_num_respones = participant_num_respones + 1\n",
    "                \n",
    "        # end of current participant's responses\n",
    "        \n",
    "        # continue to add with the following participants\n",
    "        if participant_num_respones != 0:\n",
    "            total_rmse = total_rmse + participant_rmse # add\n",
    "            total_num_responses = total_num_responses + participant_num_respones\n",
    "            # rmse for current participant\n",
    "            participant_rmse = math.sqrt(participant_rmse / participant_num_respones)\n",
    "\n",
    "        # add participants RMSE\n",
    "        accumulative_rmse = accumulative_rmse + participant_rmse\n",
    "\n",
    "        # calculate rmse of current participant\n",
    "        print(\"Num Participants responses: {}\".format(curr_participant.shape[0]))\n",
    "\n",
    "    # all participants processed\n",
    "    total_rmse = math.sqrt(total_rmse / total_num_responses)\n",
    "    \n",
    "    print(\"Total RMSE across all participants: {}\".format(total_rmse))\n",
    "    print(\"Accumulative of RMSE of each participant: {}\".format(accumulative_rmse))\n",
    "    \n",
    "    return total_rmse, accumulative_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTempRangeOccutherm(dataframe, clf, mlp=False):\n",
    "    df = dataframe.copy()\n",
    "    del df['Discrete Thermal Comfort_TA']\n",
    "    \n",
    "    # get list of participants in the test set\n",
    "    participants = list(df['Participant_No'].unique())\n",
    "\n",
    "    min_temps = []\n",
    "    max_temps = []\n",
    "    \n",
    "    # for each participant\n",
    "    for p in participants:\n",
    "        # participant temps\n",
    "        temps = []\n",
    "        \n",
    "        curr_participant = df[df['Participant_No'] == p]\n",
    "        del curr_participant['Participant_No']\n",
    "\n",
    "        if mlp:\n",
    "            scaler = StandardScaler()\n",
    "            \n",
    "            curr_participant_scaled = scaler.fit_transform(curr_participant)\n",
    "                    \n",
    "            # calculate the range temperatures at which occuTherm model predicts 0\n",
    "            occutherm_pred = clf.predict(np.array(curr_participant_scaled))\n",
    "        \n",
    "            # remapping y's from one hot encoding from MLP\n",
    "            occutherm_pred = np.array([np.argmax(item) for item in occutherm_pred])\n",
    "            occutherm_pred = occutherm_pred - 2 # back to original [-2,-1,0,+1,+2]\n",
    "        else:\n",
    "            occutherm_pred = clf.predict(np.array(curr_participant))\n",
    "            \n",
    "        # reset index\n",
    "        curr_participant = curr_participant.reset_index(drop=True)\n",
    "\n",
    "        for index, row in curr_participant.iterrows():\n",
    "            temp = row['Temperature (Fahrenheit)']\n",
    "            if occutherm_pred[index] == 0:\n",
    "                temps.append(temp)\n",
    "    \n",
    "        min_temps.append(min(temps))\n",
    "        max_temps.append(max(temps))\n",
    "        \n",
    "    total_min = max(min_temps)\n",
    "    total_max = min(max_temps)\n",
    "    \n",
    "    if total_min > total_max:\n",
    "        temp = total_min\n",
    "        total_min = total_max\n",
    "        total_max = temp\n",
    "\n",
    "    return total_min, total_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T07:13:45.590669Z",
     "start_time": "2019-06-27T07:13:45.574937Z"
    }
   },
   "outputs": [],
   "source": [
    "# From https://github.com/CenterForTheBuiltEnvironment/comfort_tool/blob/master/contrib/comfort_models.py\n",
    "def comfPMV(ta, tr, vel, rh, met, clo, wme):\n",
    "    \"\"\"\n",
    "    returns [pmv, ppd]\n",
    "    ta, air temperature (C)\n",
    "    tr, mean radiant temperature (C)\n",
    "    vel, relative air velocity (m/s)\n",
    "    rh, relative humidity (%) Used only this way to input humidity level\n",
    "    met, metabolic rate (met)\n",
    "    clo, clothing (clo)\n",
    "    wme, external work, normally around 0 (met)\n",
    "    \"\"\"\n",
    "\n",
    "    pa = rh * 10 * math.exp(16.6536 - 4030.183 / (ta + 235))\n",
    "\n",
    "    icl = 0.155 * clo  # thermal insulation of the clothing in M2K/W\n",
    "    m = met * 58.15  # metabolic rate in W/M2\n",
    "    w = wme * 58.15  # external work in W/M2\n",
    "    mw = m - w  # internal heat production in the human body\n",
    "    if (icl <= 0.078):\n",
    "        fcl = 1 + (1.29 * icl)\n",
    "    else:\n",
    "        fcl = 1.05 + (0.645 * icl)\n",
    "\n",
    "    # heat transf. coeff. by forced convection\n",
    "    hcf = 12.1 * math.sqrt(vel)\n",
    "    taa = ta + 273\n",
    "    tra = tr + 273\n",
    "    tcla = taa + (35.5 - ta) / (3.5 * icl + 0.1)\n",
    "\n",
    "    p1 = icl * fcl\n",
    "    p2 = p1 * 3.96\n",
    "    p3 = p1 * 100\n",
    "    p4 = p1 * taa\n",
    "    p5 = (308.7 - 0.028 * mw) + (p2 * math.pow(tra / 100, 4))\n",
    "    xn = tcla / 100\n",
    "    xf = tcla / 50\n",
    "    eps = 0.00015\n",
    "\n",
    "    n = 0\n",
    "    while abs(xn - xf) > eps:\n",
    "        xf = (xf + xn) / 2\n",
    "        hcn = 2.38 * math.pow(abs(100.0 * xf - taa), 0.25)\n",
    "        if (hcf > hcn):\n",
    "            hc = hcf\n",
    "        else:\n",
    "            hc = hcn\n",
    "        xn = (p5 + p4 * hc - p2 * math.pow(xf, 4)) / (100 + p3 * hc)\n",
    "        n += 1\n",
    "        if (n > 150):\n",
    "            print('Max iterations exceeded')\n",
    "            return 1\n",
    "\n",
    "    tcl = 100 * xn - 273\n",
    "\n",
    "    # heat loss diff. through skin\n",
    "    hl1 = 3.05 * 0.001 * (5733 - (6.99 * mw) - pa)\n",
    "    # heat loss by sweating\n",
    "    if mw > 58.15:\n",
    "        hl2 = 0.42 * (mw - 58.15)\n",
    "    else:\n",
    "        hl2 = 0\n",
    "    # latent respiration heat loss\n",
    "    hl3 = 1.7 * 0.00001 * m * (5867 - pa)\n",
    "    # dry respiration heat loss\n",
    "    hl4 = 0.0014 * m * (34 - ta)\n",
    "    # heat loss by radiation\n",
    "    hl5 = 3.96 * fcl * (math.pow(xn, 4) - math.pow(tra / 100, 4))\n",
    "    # heat loss by convection\n",
    "    hl6 = fcl * hc * (tcl - ta)\n",
    "\n",
    "    ts = 0.303 * math.exp(-0.036 * m) + 0.028\n",
    "    pmv = ts * (mw - hl1 - hl2 - hl3 - hl4 - hl5 - hl6)\n",
    "    ppd = 100.0 - 95.0 * math.exp(-0.03353 * pow(pmv, 4.0)\n",
    "        - 0.2179 * pow(pmv, 2.0))\n",
    "\n",
    "    r = []\n",
    "    r.append(pmv)\n",
    "    r.append(ppd)\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T04:58:00.184807Z",
     "start_time": "2019-06-30T04:58:00.166355Z"
    }
   },
   "outputs": [],
   "source": [
    "def ppv(pmv, dataframe_train, dataframe_test, vel=0.2, rh=0.5, met=1, clo=0.8, binary=False, no_clo=False):\n",
    "    \"\"\"\n",
    "    returns ppv as a linear combination of the following features:\n",
    "    ta, air temperature (C)\n",
    "    tr, mean radiant temperature (C)\n",
    "    vel, relative air velocity (m/s)\n",
    "    rh, relative humidity (%) Used only this way to input humidity level\n",
    "    met, metabolic rate (met)\n",
    "    clo, clothing (clo)\n",
    "    \n",
    "    According to SPOT paper:\n",
    "    ppv(x) = pmv(x) + personal(x)\n",
    "\n",
    "    personal(x) = a'.x + b\n",
    "\n",
    "    a={a_temp, a_radiant, a_velocity, a_humidity, a_metabolic, a_clothing}\n",
    "    \"\"\"\n",
    "    \n",
    "    ppv_train_df = pd.DataFrame()\n",
    "    ppv_test_df = pd.DataFrame()\n",
    "    \n",
    "    ppv_train_df['Temperature (Fahrenheit)'] = (dataframe_train['Temperature (Fahrenheit)']  - 32) * 5.0 / 9.0\n",
    "    ppv_train_df['tr'] = (dataframe_train['Temperature (Fahrenheit)']  - 32) * 5.0 / 9.0\n",
    "    ppv_train_df['vel'] = vel\n",
    "    ppv_train_df['rh'] = rh\n",
    "    ppv_train_df['met'] = met\n",
    "\n",
    "    ppv_test_df['Temperature (Fahrenheit)'] = (dataframe_test['Temperature (Fahrenheit)']  - 32) * 5.0 / 9.0\n",
    "    ppv_test_df['tr'] = (dataframe_test['Temperature (Fahrenheit)']  - 32) * 5.0 / 9.0\n",
    "    ppv_test_df['vel'] = vel\n",
    "    ppv_test_df['rh'] = rh\n",
    "    ppv_test_df['met'] = met\n",
    "        \n",
    "    if no_clo:\n",
    "        ppv_train_df['ClothingInsulation'] = clo\n",
    "        ppv_test_df['ClothingInsulation'] = clo\n",
    "    else:\n",
    "        ppv_train_df['ClothingInsulation'] = dataframe_train['ClothingInsulation']\n",
    "        ppv_test_df['ClothingInsulation'] = dataframe_test['ClothingInsulation']\n",
    "    \n",
    "    ppv_train_df['Discrete Thermal Comfort_TA'] = dataframe_train['Discrete Thermal Comfort_TA']\n",
    "    ppv_test_df['Discrete Thermal Comfort_TA'] = dataframe_test['Discrete Thermal Comfort_TA']\n",
    "    \n",
    "    # remapping for binary \n",
    "    if binary:\n",
    "        ppv_train_df['Discrete Thermal Comfort_TA'] = ppv_train_df['Discrete Thermal Comfort_TA'].map(lambda x: 1 if x != 0 else 0)\n",
    "        ppv_test_df['Discrete Thermal Comfort_TA'] = ppv_test_df['Discrete Thermal Comfort_TA'].map(lambda x: 1 if x != 0 else 0)\n",
    "        \n",
    "    # break down the df\n",
    "    X_train = np.array(ppv_train_df.iloc[:, 0:ppv_train_df.shape[1] - 1]) # minus 1 for the comfort label\n",
    "    y_train = np.array(ppv_train_df['Discrete Thermal Comfort_TA'])\n",
    "    \n",
    "    X_test = np.array(ppv_test_df.iloc[:, 0:ppv_test_df.shape[1] - 1]) # minus 1 for the comfort label\n",
    "    y_test = np.array(ppv_test_df['Discrete Thermal Comfort_TA'])\n",
    "        \n",
    "    # train linear regression\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train) # TODO: does this take care of boolean variables?\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "    \n",
    "    reg = LinearRegression().fit(X_train_scaled, y_train)\n",
    "\n",
    "    personal_pred = reg.predict(X_test_scaled)\n",
    "    \n",
    "    ppv_pred_dec = np.around(pmv + personal_pred, 1)\n",
    "    ppv_pred = np.around(pmv + personal_pred, 0)\n",
    "    \n",
    "    # clip to -2,+2\n",
    "    ppv_pred = np.clip(ppv_pred, a_min = -2, a_max = 2) \n",
    "    \n",
    "    # clip to 0,1\n",
    "    if binary:\n",
    "        ppv_pred = np.clip(ppv_pred, a_min = 0, a_max = 1) \n",
    "    \n",
    "    # get metrics\n",
    "    return getClfMetrics(y_test, ppv_pred), ppv_pred_dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T07:13:45.636734Z",
     "start_time": "2019-06-27T07:13:45.621069Z"
    }
   },
   "outputs": [],
   "source": [
    "def selectModelParameters(train_vectors, train_labels, trainclf, parameters, scorer, useSampleWeight = False):\n",
    "    \"\"\"\n",
    "    Choose the best combination of parameters for a given model\n",
    "    \"\"\"\n",
    "    \n",
    "    k = chooseK(train_labels) # get number of folds\n",
    "\n",
    "    stratifiedKFold = StratifiedKFold(n_splits = k)\n",
    "    if useSampleWeight:\n",
    "        n_samples = len(train_labels)\n",
    "        n_classes = len(set(train_labels))\n",
    "        classCounter = Counter(train_labels)\n",
    "        sampleWeights = [n_samples / (n_classes * classCounter[label]) for label in train_labels]\n",
    "        \n",
    "        print(\"Number of folds: \" + str(k))\n",
    "        chosen_cv = stratifiedKFold\n",
    "        \n",
    "        gridSearch = GridSearchCV(trainclf, parameters, cv = chosen_cv, scoring = scorer, \n",
    "                                      fit_params = {'sample_weight' : sampleWeights})\n",
    "    else:\n",
    "        print(\"Number of folds: \" + str(k))\n",
    "        chosen_cv = stratifiedKFold\n",
    "        \n",
    "        gridSearch = GridSearchCV(trainclf, parameters, cv = chosen_cv, scoring = scorer)\n",
    "    \n",
    "    gridSearch.fit(train_vectors, train_labels)\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print(gridSearch.best_params_)\n",
    "    \n",
    "    return gridSearch.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T07:13:45.654518Z",
     "start_time": "2019-06-27T07:13:45.638429Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainTest_tunedModel(df_test, clf_optimal, mlp_tuned=False, binary_mlp=False):\n",
    "    X_test = np.array(df_test.iloc[:, 0:df_test.shape[1] - 1]) # minus 1 for the comfort label\n",
    "    y_test = np.array(df_test['Discrete Thermal Comfort_TA'])\n",
    "\n",
    "    # remapping for mlp\n",
    "    if mlp_tuned:\n",
    "        # re-map labels from {-2, -1, 0, 1, 2} to {0, 1, 2, 3, 4}; works better with _tanh_ activation\n",
    "        y_norm_test = y_test + 2\n",
    "    \n",
    "        # binary case\n",
    "        if binary_mlp:\n",
    "            y_norm_test = df_test['Discrete Thermal Comfort_TA'].map(lambda x: 1 if x != 0 else 0)\n",
    "    \n",
    "        # generate one-hot vector representation; e.g., 0 = [1 0 0 0 0], 1 = [0 1 0 0 0], etc.\n",
    "        y_test = np.zeros((y_norm_test.size, y_norm_test.max() + 1)) \n",
    "        y_test[np.arange(y_norm_test.size),y_norm_test] = 1 \n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "        \n",
    "    #predict the response on test set\n",
    "    y_pred = clf_optimal.predict(X_test)\n",
    "\n",
    "    if mlp_tuned:\n",
    "        # remapped y's\n",
    "        y_pred = np.array([np.argmax(item) for item in y_pred])\n",
    "        y_test = np.array([np.argmax(item) for item in y_test])\n",
    "\n",
    "    # get metrics\n",
    "    return getClfMetrics(y_test, y_pred), y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T07:13:45.671765Z",
     "start_time": "2019-06-27T07:13:45.655861Z"
    }
   },
   "outputs": [],
   "source": [
    "def buildTrainRF(dataframe, test_size_percentage=0.5):\n",
    "    \"\"\"\n",
    "    Breakdown the dataframe into X and y arrays. Later split them in train and test set. Train the model with CV\n",
    "    and then find the optimal tree depth and report the accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    # create design matrix X and target vector y\n",
    "    X = np.array(dataframe.iloc[:, 0:dataframe.shape[1] - 1]) # minus 1 for the comfort label\n",
    "    y = np.array(dataframe['Discrete Thermal Comfort_TA'])\n",
    "\n",
    "    print(\"Features: {}\".format(dataframe.columns.values[:-1]))  # minus 1 for the comfort label\n",
    "\n",
    "    parameters = {'n_estimators' : [10, 100, 1000],\n",
    "                  'criterion' : ['entropy', 'gini'],\n",
    "                  'min_samples_split' : [2, 10, 20, 30], \n",
    "                  'class_weight' : ['balanced', 'balanced_subsample']}\n",
    "    scorer = 'f1_micro'\n",
    "    clf = RandomForestClassifier(n_estimators = 10, min_samples_split = 2, class_weight = 'balanced', \n",
    "                                 random_state = 100)\n",
    "    \n",
    "   \n",
    "    # split into train and test CV\n",
    "    # X_train = train + cv set (train_vectors)\n",
    "    # X_test = test set (test_vectors)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size_percentage, \n",
    "                                                        random_state = 100, stratify = y)\n",
    "    # for the holistic approach, dataframe doesn't have the Participants_No column so proceed to do gridsearchCV\n",
    "    rf_classifier = selectModelParameters(X_train, y_train, clf, parameters, scorer)\n",
    "        \n",
    "    # find optimal depth and generate model\n",
    "    optimal_depth = chooseOptimalTreeDepth(rf_classifier, X_train, y_train)\n",
    "\n",
    "    # generate the model with the selected paramters plus the optimal depth and do the model fitting\n",
    "    rf_optimal = rf_classifier.set_params(max_depth = optimal_depth)\n",
    "    print(rf_optimal)\n",
    "    \n",
    "    # fit the model\n",
    "    rf_optimal.fit(X_train, y_train)\n",
    "\n",
    "    # predict the response on test set\n",
    "    y_pred = rf_optimal.predict(X_test)\n",
    "\n",
    "    # get metrics\n",
    "    rf_acc = getClfMetrics(y_test, y_pred)\n",
    "    \n",
    "    return rf_acc, rf_optimal\n",
    "\n",
    "# Choose the optimal depth of a tree model \n",
    "def chooseOptimalTreeDepth(clf, train_vectors, train_labels, list_participants_65 = None,\n",
    "                           LOPO=False, plot = True):\n",
    "    \"\"\"\n",
    "    Choose the optimal depth of a tree model \n",
    "    \"\"\"\n",
    "    \n",
    "    DEFAULT_K = 10\n",
    "    \n",
    "    # generate a list of potential depths to calculate the optimal\n",
    "    depths = list(range(1, 25))\n",
    "\n",
    "    # empty list that will hold cv scores\n",
    "    cv_scores = []\n",
    "\n",
    "    print(\"Finding optimal tree depth\")\n",
    "    # find optimal tree depth    \n",
    "    for d in depths: # TODO: try using chooseK(train_labels) instead of jus DEFAULT_K\n",
    "        clf_depth = clf.set_params(max_depth = d) # use previous parameters while changing depth\n",
    "\n",
    "        if LOPO:\n",
    "            chosen_cv = LOPO_cv(train_vectors, list_participants_65)\n",
    "        else:\n",
    "            chosen_cv = chooseK(train_labels)\n",
    "\n",
    "        scores = cross_val_score(clf_depth, train_vectors, \n",
    "                                 train_labels, cv = chosen_cv,\n",
    "                                 scoring = 'accuracy') # accuracy here is f1 micro\n",
    "        cv_scores.append(scores.mean())\n",
    "\n",
    "    # changing to misclassification error and determining best depth\n",
    "    MSE = [1 - x for x in cv_scores] # MSE = 1 - f1_micro\n",
    "    optimal_depth = depths[MSE.index(min(MSE))]\n",
    "    print(\"The optimal depth is: \", optimal_depth, \"\\n\")\n",
    "    print(\"Expected accuracy (f1 micro) based on Cross-Validation: \", cv_scores[depths.index(optimal_depth)], \"\\n\")\n",
    "    \n",
    "    if plot:\n",
    "        # plot misclassification error vs depths\n",
    "        fig = plt.figure(figsize=(12, 10))\n",
    "        plt.plot(depths, MSE)\n",
    "        plt.xlabel('Tree Depth', fontsize = 20)\n",
    "        plt.ylabel('Misclassification Error', fontsize = 20)\n",
    "        plt.legend(fontsize = 15)\n",
    "        plt.show()\n",
    "\n",
    "    return optimal_depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T07:13:45.688034Z",
     "start_time": "2019-06-27T07:13:45.673184Z"
    }
   },
   "outputs": [],
   "source": [
    "def buildTrainKNN(dataframe, test_size_percentage=0.5):\n",
    "    \"\"\"\n",
    "    Breakdown the dataframe into X and y arrays. Later split them in train and test set. Train the model with CV\n",
    "    and report the accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    # create design matrix X and target vector y\n",
    "    X = np.array(dataframe.iloc[:, 0:dataframe.shape[1] - 1]) # minus 1 for the comfort label\n",
    "    y = np.array(dataframe['Discrete Thermal Comfort_TA'])\n",
    "\n",
    "    print(\"Features: {}\".format(dataframe.columns.values[:-1]))  # minus 1 for the comfort label\n",
    "    \n",
    "    # split into train and test\n",
    "    # X_train = train + cv set (train_vectors)\n",
    "    # X_test = test set (test_vectors)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size_percentage, random_state = 100, stratify = y)\n",
    "\n",
    "    parameters = {'n_neighbors' : [3, 5, 7, 9, 10, 11, 12, 13, 14, 15], \n",
    "                  'weights' : ['uniform', 'distance'], \n",
    "                  'metric' : ['seuclidean'], 'algorithm' : ['brute']}\n",
    "    scorer = 'f1_micro'\n",
    "    clf = KNeighborsClassifier(n_neighbors = 3, weights = 'uniform', metric = 'seuclidean', algorithm = 'brute')\n",
    "    \n",
    "    knn_classifier = selectModelParameters(X_train, y_train, clf, parameters, scorer)\n",
    "\n",
    "    # fitting the model\n",
    "    knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "    print(knn_classifier)\n",
    "\n",
    "    # predict the response\n",
    "    y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "    # evaluate accuracyt\n",
    "    knn_acc = getClfMetrics(y_test, y_pred)\n",
    "\n",
    "    return knn_acc, knn_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T07:13:45.705079Z",
     "start_time": "2019-06-27T07:13:45.689135Z"
    }
   },
   "outputs": [],
   "source": [
    "def buildTrainSVM(dataframe, test_size_percentage):\n",
    "    \"\"\"\n",
    "    Breakdown the dataframe into X and y arrays. Later split them in train and test set. Train the model with CV\n",
    "    and report the accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # create design matrix X and target vector y\n",
    "    X = np.array(dataframe.iloc[:, 0:dataframe.shape[1] - 1]) # minus 1 for the comfort label\n",
    "    y = np.array(dataframe['Discrete Thermal Comfort_TA'])\n",
    "\n",
    "    print(\"Features: {}\".format(dataframe.columns.values[:-1]))  # minus 1 for the comfort label\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_X = scaler.fit_transform(X) # TODO: does this take care of boolean variables?\n",
    "\n",
    "    # split into train and test\n",
    "\n",
    "    # X_train = train + cv set (train_vectors)\n",
    "    # X_test = test set (test_vectors)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size = test_size_percentage, random_state = 100, stratify = y)\n",
    "\n",
    "    parameters = [{'C' : [1, 10, 100, 1000],\n",
    "                   'kernel' : ['linear'], \n",
    "                   'class_weight' : ['balanced']},\n",
    "                  {'C' : [1, 10, 100, 1000], \n",
    "                   'kernel' : ['rbf'], \n",
    "                   'gamma' : [0.1, 0.01, 0.001, 0.0001], \n",
    "                   'class_weight' : ['balanced']}]\n",
    "    clf = SVC(C = 1, kernel = 'linear', class_weight = None, random_state = 100)\n",
    "    scorer = 'f1_micro'\n",
    "    svm_classifier = selectModelParameters(X_train, y_train, clf, parameters, scorer)\n",
    "\n",
    "    # fitting the model\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    print(svm_classifier)\n",
    "\n",
    "    # predict the response\n",
    "    y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "    # evaluate accuracy\n",
    "    svm_acc = getClfMetrics(y_test, y_pred)\n",
    "    \n",
    "    return svm_acc, svm_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T07:13:45.722168Z",
     "start_time": "2019-06-27T07:13:45.706246Z"
    }
   },
   "outputs": [],
   "source": [
    "def buildTrainNB(dataframe, test_size_percentage):\n",
    "    \"\"\"\n",
    "    Breakdown the dataframe into X and y arrays. Later split them in train and test set. Train the model with CV\n",
    "    and report the accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    DEFAULT_K = 10\n",
    "    \n",
    "    # create design matrix X and target vector y\n",
    "    X = np.array(dataframe.iloc[:, 0:dataframe.shape[1] - 1]) # minus 1 for the comfort label\n",
    "    y = np.array(dataframe['Discrete Thermal Comfort_TA'])\n",
    "\n",
    "    print(\"Features: {}\".format(dataframe.columns.values[:-1]))  # minus 1 for the comfort label\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_X = scaler.fit_transform(X) # TODO: does this take care of boolean variables?\n",
    "\n",
    "    # split into train and test\n",
    "\n",
    "    # X_train = train + cv set\n",
    "    # X_test = test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size = test_size_percentage, random_state = 100, stratify = y)\n",
    "\n",
    "    # instantiate learning model\n",
    "    nb_classifier = GaussianNB() # TODO get priors?\n",
    "\n",
    "    # k-fold cross validation\n",
    "    scores = cross_val_score(nb_classifier, X_train, y_train, cv = DEFAULT_K, scoring = 'accuracy') # accuracy here is f1 micro\n",
    "    print(\"Expected accuracy (f1 micro) based on Cross-Validation: \", scores.mean())\n",
    "\n",
    "    # fitting the model\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "    print(nb_classifier)\n",
    "\n",
    "    # predict the response\n",
    "    y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    nb_acc = getClfMetrics(y_test, y_pred)\n",
    "    \n",
    "    return nb_acc, nb_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T05:30:10.073198Z",
     "start_time": "2019-06-29T05:30:10.060966Z"
    }
   },
   "outputs": [],
   "source": [
    "def buildMLP(dataframe, binary=False):\n",
    "    \"\"\"\n",
    "    Breakdown the dataframe into X and y arrays. Later split them in train and test set. Train the model with CV\n",
    "    and report the accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    DEFAULT_K = 10\n",
    "    \n",
    "    # create design matrix X and target vector y\n",
    "    X = np.array(dataframe.iloc[:, 0:dataframe.shape[1] - 1]) # minus 1 for the comfort label\n",
    "    y_original = np.array(dataframe['Discrete Thermal Comfort_TA'])\n",
    "\n",
    "    # re-map labels from {-2, -1, 0, 1, 2} to {0, 1, 2, 3, 4}; works better with _tanh_ activation\n",
    "    y_norm = y_original + 2\n",
    "    \n",
    "    # binary case\n",
    "    if binary:\n",
    "        y_norm = dataframe['Discrete Thermal Comfort_TA'].map(lambda x: 1 if x != 0 else 0)\n",
    "    \n",
    "    # generate one-hot vector representation; e.g., 0 = [1 0 0 0 0], 1 = [0 1 0 0 0], etc.\n",
    "    y = np.zeros((y_norm.size, y_norm.max()+1)) \n",
    "    y[np.arange(y_norm.size),y_norm] = 1 \n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_X = scaler.fit_transform(X) # TODO: does this take care of boolean variables?\n",
    "\n",
    "    # split into train and test\n",
    "    test_size_percentage = 0.2 # standard 80/20 split\n",
    "\n",
    "    # X_train = train + cv set\n",
    "    # X_test = test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size = test_size_percentage, random_state = 100, stratify = y)\n",
    "\n",
    "    # MLP is sensitive to feature scaling. \n",
    "    if not binary:\n",
    "        mlp = MLPClassifier(hidden_layer_sizes = (250,100,25,5), \n",
    "                        activation = 'tanh', \n",
    "                        solver = 'adam',\n",
    "                        learning_rate = 'adaptive',\n",
    "                        alpha = 1e-4, \n",
    "                        learning_rate_init = 1e-3, # only used with solver adam or sgd\n",
    "                        max_iter = 1000,\n",
    "                        tol = 1e-8,\n",
    "                        batch_size = 5,\n",
    "                        random_state = 100,\n",
    "                        verbose = True,\n",
    "                        warm_start = False,\n",
    "                        nesterovs_momentum = True,\n",
    "                        shuffle = True)\n",
    "\n",
    "    else:\n",
    "        mlp = MLPClassifier(hidden_layer_sizes = (250,100,25,2), \n",
    "                activation = 'tanh', \n",
    "                solver = 'adam',\n",
    "                learning_rate = 'adaptive',\n",
    "                alpha = 1e-4, \n",
    "                learning_rate_init = 1e-3, # only used with solver adam or sgd\n",
    "                max_iter = 1000,\n",
    "                tol = 1e-8,\n",
    "                batch_size = 5,\n",
    "                random_state = 100,\n",
    "                verbose = True,\n",
    "                warm_start = False,\n",
    "                nesterovs_momentum = True,\n",
    "                shuffle = True)\n",
    "    \n",
    "    # model learned\n",
    "    print(mlp)\n",
    "\n",
    "    # k-fold cross validation\n",
    "    scores = cross_val_score(mlp, X_train, y_train, cv = DEFAULT_K, scoring = 'accuracy') # accuracy here is f1 micro\n",
    "    print(\"Expected accuracy (f1 micro) based on Cross-Validation: \", scores.mean(), \"\\n\")\n",
    "\n",
    "    # fitting the model\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    # predict the response\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    \n",
    "    # remapped y's\n",
    "    y_pred_val = np.array([np.argmax(item) for item in y_pred])\n",
    "    y_test_val = np.array([np.argmax(item) for item in y_test])\n",
    "    \n",
    "    # Metrics\n",
    "    mlp_acc = getClfMetrics(y_test_val, y_pred_val)\n",
    "    return mlp_acc, mlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T11:20:12.872981Z",
     "start_time": "2019-06-27T11:20:12.866614Z"
    }
   },
   "outputs": [],
   "source": [
    "def saveModel(fileName, model):\n",
    "    pickle.dump(model, open(fileName, 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
